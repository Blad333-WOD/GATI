{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c49c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: traci in c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.0)\n",
      "Requirement already satisfied: sumolib>=1.24.0 in c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from traci) (1.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\yashg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1531144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yashg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, time\n",
    "import traci\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTrafficEnvironment:\n",
    "    def __init__(self, data, n_junctions=2, max_steps=100):\n",
    "        self.data = data\n",
    "        self.n_junctions = n_junctions\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.vehicle_directions = 4  # North, South, East, West\n",
    "        self.weather_categories = 5  # Clear, Rain, Fog, Snow, Storm\n",
    "        \n",
    "\n",
    "        self.state_size = (4 + 4 + 1 + 1 + 5) * n_junctions\n",
    "        self.action_size = 9  \n",
    "        \n",
    "        # Weather encoding dictionary\n",
    "        self.weather_encoding = {\n",
    "            'clear': [1, 0, 0, 0, 0],\n",
    "            'rain': [0, 1, 0, 0, 0],\n",
    "            'fog': [0, 0, 1, 0, 0],\n",
    "            'snow': [0, 0, 0, 1, 0],\n",
    "            'storm': [0, 0, 0, 0, 1]\n",
    "        }\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.junction_data = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_junctions):\n",
    "            idx = np.random.randint(0, len(self.data))\n",
    "            junction = self.data.iloc[idx].copy()\n",
    "            \n",
    "            \n",
    "            current_hour = np.random.randint(0, 24)\n",
    "            junction['time_of_day'] = current_hour / 24.0\n",
    "            \n",
    "            # Add weather condition\n",
    "            weather_conditions = ['clear', 'rain', 'fog', 'snow', 'storm']\n",
    "            junction['weather'] = np.random.choice(weather_conditions, p=[0.6, 0.2, 0.1, 0.05, 0.05])\n",
    "            \n",
    "            self.junction_data.append(junction)\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \n",
    "        state = []\n",
    "        \n",
    "        for junction in self.junction_data:\n",
    "            \n",
    "            vehicle_counts = [\n",
    "                junction.get('north_count', np.random.randint(0, 50)),\n",
    "                junction.get('south_count', np.random.randint(0, 50)),\n",
    "                junction.get('east_count', np.random.randint(0, 50)),\n",
    "                junction.get('west_count', np.random.randint(0, 50))\n",
    "            ]\n",
    "            \n",
    "            # Queue lengths \n",
    "            queue_lengths = [\n",
    "                junction.get('north_queue', np.random.randint(0, 20)),\n",
    "                junction.get('south_queue', np.random.randint(0, 20)),\n",
    "                junction.get('east_queue', np.random.randint(0, 20)),\n",
    "                junction.get('west_queue', np.random.randint(0, 20))\n",
    "            ]\n",
    "            \n",
    "            # Time of day \n",
    "            time_of_day = [junction['time_of_day']]\n",
    "            \n",
    "            # Traffic density \n",
    "            traffic_density = [junction.get('traffic_density', np.random.uniform(0, 100)) / 100.0]\n",
    "            \n",
    "            # Weather \n",
    "            weather = self.weather_encoding[junction['weather']]\n",
    "            \n",
    "            # Combine all features\n",
    "            junction_state = vehicle_counts + queue_lengths + time_of_day + traffic_density + weather\n",
    "            state.extend(junction_state)\n",
    "        \n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        ns_green, ew_green = self._action_to_timings(action)\n",
    "        \n",
    "        # Calculate comprehensive reward\n",
    "        reward = self._calculate_enhanced_reward(ns_green, ew_green)\n",
    "        \n",
    "        # Update state\n",
    "        self._update_state()\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        info = {\n",
    "            'ns_green': ns_green,\n",
    "            'ew_green': ew_green,\n",
    "            'total_vehicles': self._get_total_vehicles(),\n",
    "            'total_queue': self._get_total_queue()\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _action_to_timings(self, action):\n",
    "        \"\"\"Convert action to signal timings\"\"\"\n",
    "        timing_combinations = [\n",
    "            (20, 40), (25, 35), (30, 30),  \n",
    "            (35, 25), (40, 20), (45, 45),  \n",
    "            (50, 40), (55, 35), (60, 30)   \n",
    "        ]\n",
    "        return timing_combinations[action]\n",
    "    \n",
    "    def _calculate_enhanced_reward(self, ns_green, ew_green):\n",
    "        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for junction in self.junction_data:\n",
    "            \n",
    "            total_vehicles = (junction.get('north_count', 0) + junction.get('south_count', 0) + \n",
    "                            junction.get('east_count', 0) + junction.get('west_count', 0))\n",
    "            total_queue = (junction.get('north_queue', 0) + junction.get('south_queue', 0) + \n",
    "                          junction.get('east_queue', 0) + junction.get('west_queue', 0))\n",
    "            \n",
    "            # Throughput reward \n",
    "            throughput_reward = total_vehicles * 0.5\n",
    "            \n",
    "            # Queue penalty \n",
    "            queue_penalty = total_queue * -1.0\n",
    "            \n",
    "            # Traffic density penalty\n",
    "            density_penalty = junction.get('traffic_density', 0) * -0.1\n",
    "            \n",
    "            time_factor = 1.0\n",
    "            if 7 <= junction['time_of_day'] * 24 <= 9 or 17 <= junction['time_of_day'] * 24 <= 19:\n",
    "                time_factor = 1.5 \n",
    "            \n",
    "            \n",
    "            weather_penalty = 0\n",
    "            if junction['weather'] in ['rain', 'fog', 'storm']:\n",
    "                weather_penalty = -5.0  \n",
    "            \n",
    "            # Signal timing efficiency\n",
    "            timing_efficiency = 0\n",
    "            if 25 <= ns_green + ew_green <= 75: \n",
    "                timing_efficiency = 10\n",
    "            \n",
    "            junction_reward = (throughput_reward + queue_penalty + density_penalty + \n",
    "                             timing_efficiency + weather_penalty) * time_factor\n",
    "            total_reward += junction_reward\n",
    "        \n",
    "        return total_reward / self.n_junctions\n",
    "    \n",
    "    def _update_state(self):\n",
    "        for junction in self.junction_data:\n",
    "            \n",
    "            for direction in ['north_count', 'south_count', 'east_count', 'west_count']:\n",
    "                if direction in junction:\n",
    "                    junction[direction] = max(0, junction[direction] + np.random.randint(-3, 4))\n",
    "            \n",
    "            # Dynamic queue changes\n",
    "            for queue in ['north_queue', 'south_queue', 'east_queue', 'west_queue']:\n",
    "                if queue in junction:\n",
    "                    junction[queue] = max(0, junction[queue] + np.random.randint(-2, 3))\n",
    "            \n",
    "            # Update traffic density\n",
    "            junction['traffic_density'] = max(0, min(100, \n",
    "                junction.get('traffic_density', 0) + np.random.normal(0, 3)))\n",
    "    \n",
    "    def _get_total_vehicles(self):\n",
    "\n",
    "        total = 0\n",
    "        for junction in self.junction_data:\n",
    "            total += sum([junction.get(f'{dir}_count', 0) \n",
    "                         for dir in ['north', 'south', 'east', 'west']])\n",
    "        return total\n",
    "    \n",
    "    def _get_total_queue(self):\n",
    "        total = 0\n",
    "        for junction in self.junction_data:\n",
    "            total += sum([junction.get(f'{dir}_queue', 0) \n",
    "                         for dir in ['north', 'south', 'east', 'west']])\n",
    "        return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.0005):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)  # Larger memory\n",
    "        \n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.05  \n",
    "        self.epsilon_decay = 0.998  \n",
    "        self.exploration_steps = 200  \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = 0.95  \n",
    "        self.batch_size = 64  \n",
    "        \n",
    "        # Build networks\n",
    "        self.q_network = self._build_model()\n",
    "        self.target_network = self._build_model()\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Training step counter\n",
    "        self.train_step = 0\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build improved neural network\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, input_dim=self.state_size, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            \n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='huber')  # Huber loss for stability\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \n",
    "        tau = 0.005  \n",
    "        target_weights = self.target_network.get_weights()\n",
    "        main_weights = self.q_network.get_weights()\n",
    "        \n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = tau * main_weights[i] + (1 - tau) * target_weights[i]\n",
    "        \n",
    "        self.target_network.set_weights(target_weights)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience with priority\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.train_step < self.exploration_steps:\n",
    "            self.epsilon = 1.0 - (self.train_step / self.exploration_steps) * (1.0 - 0.3)\n",
    "        elif self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        \n",
    "        q_values = self.q_network.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Enhanced experience replay\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.array([e[0] for e in batch])\n",
    "        actions = np.array([e[1] for e in batch])\n",
    "        rewards = np.array([e[2] for e in batch])\n",
    "        next_states = np.array([e[3] for e in batch])\n",
    "        dones = np.array([e[4] for e in batch])\n",
    "        \n",
    "        current_q_values = self.q_network.predict(states, verbose=0)\n",
    "        next_q_values_main = self.q_network.predict(next_states, verbose=0)\n",
    "        next_q_values_target = self.target_network.predict(next_states, verbose=0)\n",
    "        \n",
    "        # Compute targets\n",
    "        targets = current_q_values.copy()\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Double DQN update\n",
    "                best_action = np.argmax(next_q_values_main[i])\n",
    "                targets[i][actions[i]] = rewards[i] + self.gamma * next_q_values_target[i][best_action]\n",
    "        \n",
    "        # Train network\n",
    "        self.q_network.fit(states, targets, epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_target_network()\n",
    "        self.train_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_rl_with_cv(df, episodes=100, n_junctions=2, min_green_time=15, cv_folds=5):\n",
    "\n",
    "    \n",
    "    print(f\"ðŸš¦ Training Enhanced Multi-Junction RL System with Cross-Validation\")\n",
    "    print(f\"Junctions: {n_junctions}, Episodes: {episodes}, Min Green Time: {min_green_time}s\")\n",
    "    print(f\"State Space: Vehicle counts + Queue lengths + Time + Density + Weather\")\n",
    "    \n",
    "    # Split data for cross-validation\n",
    "    data_size = len(df)\n",
    "    fold_size = data_size // cv_folds\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold in range(cv_folds):\n",
    "        print(f\"\\n--- Cross-Validation Fold {fold + 1}/{cv_folds} ---\")\n",
    "        \n",
    "        # Create train/test split\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = start_idx + fold_size if fold < cv_folds - 1 else data_size\n",
    "        test_data = df.iloc[start_idx:end_idx]\n",
    "        train_data = df.drop(df.index[start_idx:end_idx])\n",
    "        \n",
    "        print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n",
    "        \n",
    "        # Create environment and agent for this fold\n",
    "        env = EnhancedTrafficEnvironment(train_data, n_junctions=n_junctions, min_green_time=min_green_time)\n",
    "        agent = ImprovedDQNAgent(env.state_size, env.action_size)\n",
    "        \n",
    "        print(f\"State Size: {env.state_size}, Action Size: {env.action_size}\")\n",
    "        \n",
    "        # Training metrics for this fold\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        epsilon_history = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "            \n",
    "            for step in range(env.max_steps):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                \n",
    "                # Train agent\n",
    "                if len(agent.memory) > agent.batch_size and step % 4 == 0:\n",
    "                    agent.replay()\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_lengths.append(step_count)\n",
    "            epsilon_history.append(agent.epsilon)\n",
    "            \n",
    "            # Progress reporting\n",
    "            if episode % 20 == 0:\n",
    "                recent_reward = np.mean(episode_rewards[-10:])\n",
    "                print(f\"  Episode {episode:3d}/{episodes}, \"\n",
    "                      f\"Avg Reward: {recent_reward:7.2f}, \"\n",
    "                      f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        test_env = EnhancedTrafficEnvironment(test_data, n_junctions=n_junctions, min_green_time=min_green_time)\n",
    "        test_rewards = []\n",
    "        \n",
    "        for _ in range(10):  # 10 test episodes\n",
    "            state = test_env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            for step in range(test_env.max_steps):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, info = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            test_rewards.append(total_reward)\n",
    "        \n",
    "        fold_score = np.mean(test_rewards)\n",
    "        cv_scores.append(fold_score)\n",
    "        print(f\"  Fold {fold + 1} Test Score: {fold_score:.2f}\")\n",
    "    \n",
    "    # Final training on full dataset\n",
    "    print(f\"\\n--- Final Training on Full Dataset ---\")\n",
    "    final_env = EnhancedTrafficEnvironment(df, n_junctions=n_junctions, min_green_time=min_green_time)\n",
    "    final_agent = ImprovedDQNAgent(final_env.state_size, final_env.action_size)\n",
    "    \n",
    "    final_episode_rewards = []\n",
    "    final_episode_lengths = []\n",
    "    final_epsilon_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = final_env.reset()\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        for step in range(final_env.max_steps):\n",
    "            action = final_agent.act(state)\n",
    "            next_state, reward, done, info = final_env.step(action)\n",
    "            \n",
    "            final_agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Train agent\n",
    "            if len(final_agent.memory) > final_agent.batch_size and step % 4 == 0:\n",
    "                final_agent.replay()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        final_episode_rewards.append(total_reward)\n",
    "        final_episode_lengths.append(step_count)\n",
    "        final_epsilon_history.append(final_agent.epsilon)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if episode % 10 == 0:\n",
    "            recent_reward = np.mean(final_episode_rewards[-10:])\n",
    "            print(f\"Episode {episode:3d}/{episodes}, \"\n",
    "                  f\"Avg Reward: {recent_reward:7.2f}, \"\n",
    "                  f\"Epsilon: {final_agent.epsilon:.3f}, \"\n",
    "                  f\"Steps: {step_count}\")\n",
    "    \n",
    "    # Results summary\n",
    "    final_reward = np.mean(final_episode_rewards[-10:])\n",
    "    cv_mean = np.mean(cv_scores)\n",
    "    cv_std = np.std(cv_scores)\n",
    "    \n",
    "    print(f\"\\nâœ… Training Complete!\")\n",
    "    print(f\"Cross-Validation Results:\")\n",
    "    print(f\"  Mean Score: {cv_mean:.2f} Â± {cv_std:.2f}\")\n",
    "    print(f\"  Individual Fold Scores: {[f'{score:.2f}' for score in cv_scores]}\")\n",
    "    print(f\"Final Training Reward: {final_reward:.2f}\")\n",
    "    print(f\"Final Epsilon: {final_agent.epsilon:.3f}\")\n",
    "    print(f\"Total Training Steps: {final_agent.train_step}\")\n",
    "    print(f\"Minimum Green Time Enforced: {min_green_time}s\")\n",
    "    \n",
    "    return final_agent, final_episode_rewards, final_episode_lengths, final_epsilon_history, cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130aa46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " RL TRAFFIC MANAGEMENT POLICY DETAILS\n",
      "============================================================\n",
      "\n",
      " STATE SPACE (per junction):\n",
      "  â€¢ Vehicle Counts: [North, South, East, West] (4 values)\n",
      "  â€¢ Queue Lengths: [North, South, East, West] (4 values)\n",
      "  â€¢ Time of Day: Normalized 0-1 (1 value)\n",
      "  â€¢ Traffic Density: Normalized 0-1 (1 value)\n",
      "  â€¢ Weather: One-hot [Clear, Rain, Fog, Snow, Storm] (5 values)\n",
      "  âž¤ Total per junction: 15 features\n",
      "  âž¤ For N junctions: 15Ã—N features\n",
      "\n",
      " ACTION SPACE:\n",
      "  9 Signal Timing Combinations:\n",
      "    0: (20s NS, 40s EW)  1: (25s NS, 35s EW)  2: (30s NS, 30s EW)\n",
      "    3: (35s NS, 25s EW)  4: (40s NS, 20s EW)  5: (45s NS, 45s EW)\n",
      "    6: (50s NS, 40s EW)  7: (55s NS, 35s EW)  8: (60s NS, 30s EW)\n",
      "\n",
      " NEURAL NETWORK ARCHITECTURE:\n",
      "  â€¢ Input Layer: State size (15Ã—N neurons)\n",
      "  â€¢ Hidden Layer 1: 128 neurons + BatchNorm + Dropout(0.3)\n",
      "  â€¢ Hidden Layer 2: 64 neurons + BatchNorm + Dropout(0.3)\n",
      "  â€¢ Hidden Layer 3: 32 neurons + Dropout(0.2)\n",
      "  â€¢ Output Layer: 9 neurons (Q-values for actions)\n",
      "  â€¢ Activation: ReLU (hidden), Linear (output)\n",
      "  â€¢ Loss Function: Huber Loss\n",
      "  â€¢ Optimizer: Adam (lr=0.0005)\n",
      "\n",
      " TRAINING ALGORITHM:\n",
      "  â€¢ Algorithm: Double Deep Q-Network (DDQN)\n",
      "  â€¢ Experience Replay: 10,000 memory buffer\n",
      "  â€¢ Batch Size: 64\n",
      "  â€¢ Target Network: Soft updates (Ï„=0.005)\n",
      "  â€¢ Discount Factor (Î³): 0.95\n",
      "\n",
      " EXPLORATION STRATEGY:\n",
      "  â€¢ Policy: Improved Îµ-greedy\n",
      "  â€¢ Initial Îµ: 1.0 (100% exploration)\n",
      "  â€¢ Minimum Îµ: 0.05 (5% exploration)\n",
      "  â€¢ Decay: Linear for 200 steps, then exponential (0.998)\n",
      "  â€¢ Maintains exploration throughout training\n",
      "\n",
      " REWARD FUNCTION:\n",
      "  â€¢ Throughput: +0.5 Ã— total_vehicles\n",
      "  â€¢ Queue Penalty: -1.0 Ã— total_queue_length\n",
      "  â€¢ Density Penalty: -0.1 Ã— traffic_density\n",
      "  â€¢ Timing Efficiency: +10 (optimal signal timing)\n",
      "  â€¢ Weather Penalty: -5.0 (adverse conditions)\n",
      "  â€¢ Rush Hour Bonus: 1.5Ã— multiplier\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myashg\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSIH25\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmumbai_traffic_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m traffic records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Train enhanced model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def print_policy_info():\n",
    "    \"\"\"Print detailed policy information\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" RL TRAFFIC MANAGEMENT POLICY DETAILS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n STATE SPACE (per junction):\")\n",
    "    print(\"  â€¢ Vehicle Counts: [North, South, East, West] (4 values)\")\n",
    "    print(\"  â€¢ Queue Lengths: [North, South, East, West] (4 values)\")\n",
    "    print(\"  â€¢ Time of Day: Normalized 0-1 (1 value)\")\n",
    "    print(\"  â€¢ Traffic Density: Normalized 0-1 (1 value)\")\n",
    "    print(\"  â€¢ Weather: One-hot [Clear, Rain, Fog, Snow, Storm] (5 values)\")\n",
    "    print(\"  âž¤ Total per junction: 15 features\")\n",
    "    print(\"  âž¤ For N junctions: 15Ã—N features\")\n",
    "    \n",
    "    print(\"\\n ACTION SPACE:\")\n",
    "    print(\"  9 Signal Timing Combinations:\")\n",
    "    print(\"    0: (20s NS, 40s EW)  1: (25s NS, 35s EW)  2: (30s NS, 30s EW)\")\n",
    "    print(\"    3: (35s NS, 25s EW)  4: (40s NS, 20s EW)  5: (45s NS, 45s EW)\")\n",
    "    print(\"    6: (50s NS, 40s EW)  7: (55s NS, 35s EW)  8: (60s NS, 30s EW)\")\n",
    "    \n",
    "    print(\"\\n NEURAL NETWORK ARCHITECTURE:\")\n",
    "    print(\"  â€¢ Input Layer: State size (15Ã—N neurons)\")\n",
    "    print(\"  â€¢ Hidden Layer 1: 128 neurons + BatchNorm + Dropout(0.3)\")\n",
    "    print(\"  â€¢ Hidden Layer 2: 64 neurons + BatchNorm + Dropout(0.3)\")\n",
    "    print(\"  â€¢ Hidden Layer 3: 32 neurons + Dropout(0.2)\")\n",
    "    print(\"  â€¢ Output Layer: 9 neurons (Q-values for actions)\")\n",
    "    print(\"  â€¢ Activation: ReLU (hidden), Linear (output)\")\n",
    "    print(\"  â€¢ Loss Function: Huber Loss\")\n",
    "    print(\"  â€¢ Optimizer: Adam (lr=0.0005)\")\n",
    "    \n",
    "    print(\"\\n TRAINING ALGORITHM:\")\n",
    "    print(\"  â€¢ Algorithm: Double Deep Q-Network (DDQN)\")\n",
    "    print(\"  â€¢ Experience Replay: 10,000 memory buffer\")\n",
    "    print(\"  â€¢ Batch Size: 64\")\n",
    "    print(\"  â€¢ Target Network: Soft updates (Ï„=0.005)\")\n",
    "    print(\"  â€¢ Discount Factor (Î³): 0.95\")\n",
    "    \n",
    "    print(\"\\n EXPLORATION STRATEGY:\")\n",
    "    print(\"  â€¢ Policy: Improved Îµ-greedy\")\n",
    "    print(\"  â€¢ Initial Îµ: 1.0 (100% exploration)\")\n",
    "    print(\"  â€¢ Minimum Îµ: 0.05 (5% exploration)\")\n",
    "    print(\"  â€¢ Decay: Linear for 200 steps, then exponential (0.998)\")\n",
    "    print(\"  â€¢ Maintains exploration throughout training\")\n",
    "    \n",
    "    print(\"\\n REWARD FUNCTION:\")\n",
    "    print(\"  â€¢ Throughput: +0.5 Ã— total_vehicles\")\n",
    "    print(\"  â€¢ Queue Penalty: -1.0 Ã— total_queue_length\")\n",
    "    print(\"  â€¢ Density Penalty: -0.1 Ã— traffic_density\")\n",
    "    print(\"  â€¢ Timing Efficiency: +10 (optimal signal timing)\")\n",
    "    print(\"  â€¢ Weather Penalty: -5.0 (adverse conditions)\")\n",
    "    print(\"  â€¢ Rush Hour Bonus: 1.5Ã— multiplier\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print_policy_info()\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_csv(r'C:\\Users\\yashg\\Desktop\\SIH25\\files\\mumbai_traffic_dataset.csv')\n",
    "        print(f\"Dataset loaded: {len(df)} traffic records\")\n",
    "        \n",
    "        # Train enhanced model\n",
    "        trained_agent, rewards, lengths, epsilons = train_enhanced_rl(\n",
    "            df, episodes=80, n_junctions=2\n",
    "        )\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(rewards)\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(lengths)\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epsilons)\n",
    "        plt.title('Exploration Rate (Îµ)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.grid(True)\n",
    "        plt.axhline(y=0.05, color='r', linestyle='--', label='Min Îµ=0.05')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\" Dataset file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0a486de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic lights found: 22\n",
      "Traffic light IDs: ('1468337691', '2629418499', '471566319', 'GS_cluster_471591707_5353822010', 'cluster_10089667078_10089667079_346644272_7206407288', 'cluster_10272736790_1936355361_2629418478_296362019_#1more', 'cluster_10273014622_245664667', 'cluster_10770749309_10921550007_346633203', 'cluster_10882715558_10882715559_10882727459_6072223940', 'cluster_10882727464_10882727465_4328035970_9916444554', 'cluster_12373493651_347106948_9681688981_9681689002', 'cluster_1936355393_471554800', 'cluster_1936355394_245667457', 'cluster_2629418501_347194070_9979171948_9979171957', 'cluster_346633216_9681688983', 'cluster_347115324_5353822012', 'cluster_347149014_4310550234', 'cluster_4328035957_9739950371_9739950372_9739950373', 'cluster_5887563939_5887563940', 'joinedS_10603240106_10603240107_cluster_10603240108_10603240109', 'joinedS_1936355384_2244797448_cluster_245667153_9979171956_9979171964_9979171965', 'joinedS_2629418495_cluster_10878501990_10878501991_347176149')\n",
      "\n",
      "Details for 1468337691:\n",
      "Current phase: 0\n",
      "Phase duration: 25.0s\n"
     ]
    }
   ],
   "source": [
    "import traci\n",
    "import os\n",
    "\n",
    "# Path to your SUMO binary (update if different)\n",
    "sumo_binary = r\"C:\\Program Files (x86)\\Eclipse\\Sumo\\bin\\sumo.exe\"\n",
    "\n",
    "# Your SUMO config file\n",
    "sumo_config = r\"C:\\Users\\yashg\\Desktop\\SIH25\\mumbai\\sumo\\osm.sumocfg\"\n",
    "\n",
    "if os.path.exists(sumo_config):\n",
    "    try:\n",
    "        # Start SUMO with full path\n",
    "        traci.start([sumo_binary, \"-c\", sumo_config])\n",
    "        \n",
    "        # Get all traffic light IDs\n",
    "        tl_ids = traci.trafficlight.getIDList()\n",
    "        print(f\"Traffic lights found: {len(tl_ids)}\")\n",
    "        print(\"Traffic light IDs:\", tl_ids)\n",
    "\n",
    "        # Example: Get info for first traffic light\n",
    "        if tl_ids:\n",
    "            first_tl = tl_ids[0]\n",
    "            print(f\"\\nDetails for {first_tl}:\")\n",
    "            print(f\"Current phase: {traci.trafficlight.getPhase(first_tl)}\")\n",
    "            print(f\"Phase duration: {traci.trafficlight.getPhaseDuration(first_tl)}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        traci.close()\n",
    "else:\n",
    "    print(f\"Config file not found: {sumo_config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUMOTrafficEnvironment:\n",
    "    def __init__(self, sumo_config):\n",
    "        self.sumo_config = sumo_config\n",
    "        traci.start([\"sumo\", \"-c\", sumo_config])\n",
    "    \n",
    "    def apply_rl_action(self, junction_id, action):\n",
    "        # Convert RL action to signal timing\n",
    "        ns_green, ew_green = self.action_to_timing(action)\n",
    "        \n",
    "        # Apply to SUMO\n",
    "        traci.trafficlight.setPhaseDuration(junction_id, ns_green)\n",
    "        # Switch to EW phase\n",
    "        traci.trafficlight.setPhase(junction_id, 1)\n",
    "        traci.trafficlight.setPhaseDuration(junction_id, ew_green)\n",
    "    \n",
    "    def get_traffic_state(self, junction_id):\n",
    "        # Get real-time traffic data from SUMO\n",
    "        vehicle_count = traci.junction.getLastStepVehicleNumber(junction_id)\n",
    "        waiting_time = traci.junction.getWaitingTime(junction_id)\n",
    "        return vehicle_count, waiting_time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
